module.exports=[18622,(a,b,c)=>{b.exports=a.x("next/dist/compiled/next-server/app-page-turbo.runtime.prod.js",()=>require("next/dist/compiled/next-server/app-page-turbo.runtime.prod.js"))},42602,(a,b,c)=>{"use strict";b.exports=a.r(18622)},87924,(a,b,c)=>{"use strict";b.exports=a.r(42602).vendored["react-ssr"].ReactJsxRuntime},72131,(a,b,c)=>{"use strict";b.exports=a.r(42602).vendored["react-ssr"].React},35112,(a,b,c)=>{"use strict";b.exports=a.r(42602).vendored["react-ssr"].ReactDOM},84509,a=>{a.v([{id:"doc-intelligence-rag",title:"AI Document Intelligence & RAG Platform",summary:"Enterprise-grade document search and QA using RAG",description:"Designed and delivered an AI-powered document intelligence platform enabling semantic search, document QA, and workflow automation for enterprise content management systems.",bullets:["Developed RESTful APIs using FastAPI and Pydantic for real-time and batch document processing","Designed and implemented end-to-end RAG pipelines for AI-powered enterprise search","Integrated Typesense for high-performance vector and hybrid search","Built orchestration and retrieval workflows using LangChain","Implemented OCR pipelines using PyMuPDF, PyTesseract, and Azure Cognitive Services","Automated ETL pipelines for ingesting documents into vector databases","Developed a context-aware Document QA chatbot with conversation history retention","Integrated OpenAI APIs and fine-tuned LLM parameters for optimal retrieval and response quality","Built Streamlit-based demos for showcasing AI document intelligence capabilities","Deployed solutions using Docker, Windows Services, and on-prem infrastructure","Conducted POCs on AWS Bedrock for summarization and enterprise email analysis"],tech:["Python","FastAPI","Pydantic","LangChain","OpenAI APIs","Typesense","OCR","Docker","AWS Bedrock","SQL","Streamlit"],github:"s",demo:"s",image:"/ai.jpg"},{id:"ott-recommendation-ml",title:"OTT Recommendation & Analytics Platform",summary:"Scalable ML pipelines for content personalization",description:"Built large-scale data pipelines and recommendation systems for premium OTT streaming platforms, supporting real-time personalization, analytics, and ML-driven insights.",bullets:["Developed scalable batch and streaming data pipelines on GCP using Dataflow","Built content and user-based recommendation systems (Because You Watched, Popular Content)","Integrated Snowflake with BigQuery and Cloud Storage for analytics and ML workflows","Developed and fine-tuned T5-based ranking models for content recommendations","Deployed ML models using Vertex AI for batch and real-time inference","Implemented MLOps pipelines using Kubeflow and Vertex AI Pipelines","Deployed ML workloads on GKE with autoscaling and CI/CD using GitOps and Helm","Architected real-time pipelines using Pub/Sub, Cloud Functions, and Cloud Datastore","Built audit trail systems to track user interactions and detect anomalies","Created Looker and Tableau dashboards for engagement, churn, and content performance","Automated report generation and delivery using Cloud Scheduler and Cloud Functions"],tech:["GCP","BigQuery","Dataflow","Vertex AI","Kubeflow","Kubernetes","Snowflake","Pub/Sub","Python","SQL","Looker","Tableau","Streamlit"],github:"s",demo:"s",image:"/re.jpg"},{id:"enterprise-doc-ml",title:"Enterprise Document Classification & Search",summary:"ML-driven document classification and analytics",description:"Developed machine learning solutions to classify, organize, and analyze enterprise documents, improving accessibility and search efficiency before migration to DMS platforms.",bullets:["Built ML-based document classification systems using Random Forest and SVM","Implemented NLP techniques for content-based document categorization","Used K-means clustering for unsupervised document organization","Designed ETL pipelines to ingest documents into Elasticsearch","Developed Python scripts to automate data extraction and preprocessing","Created real-time document analytics dashboards for metadata and status tracking","Performed load testing and automated test case generation for ML pipelines"],tech:["Python","NLP","Random Forest","SVM","K-means","Elasticsearch","Kibana","SQL","PostgreSQL"],github:"s",demo:"s",image:"/dp.jpg"},{id:"ai-recommendation-vision",title:"AI Recommendation & Computer Vision Systems",summary:"NLP and computer vision-based content intelligence",description:"Developed AI-powered recommendation and content moderation systems using NLP and deep learning techniques.",bullets:["Designed and built a text recommendation system using NLP and Keras","Developed an AI-based nudity detection system using CNNs for content moderation","Worked with large-scale structured and unstructured datasets","Implemented deep learning models for image and text classification","Performed unit testing, system testing, and UAT for ML applications","Participated in full SDLC from requirements to production support"],tech:["Python","Keras","Deep Learning","CNN","NLP","Image Processing","MongoDB","Linux"],github:"s",demo:"s",image:"/cv.jpg"}])},94111,a=>{a.v([{id:"mycoir-political-intelligence",title:"MyCoir – Political Intelligence & Activity Tracking Platform",summary:"Civic intelligence platform for political member tracking and public engagement",description:"Designed and developed a full-stack political intelligence platform to manage, monitor, and showcase political representatives’ activities across state, district, taluk, subdivision, and village levels, enabling transparency, public participation, and data-driven governance.",bullets:["Designed hierarchical data models for state, district, taluk, subdivision, and village-level political representation","Built role-based dashboards to track political members’ activities, initiatives, and performance metrics","Implemented content publishing features for announcements, updates, and constituency engagement","Developed public issue reporting and polling modules to collect citizen feedback and opinions","Built real-time dashboards visualizing activity completion, engagement metrics, and constituency coverage","Implemented approval workflows and moderation mechanisms for public posts and issues","Designed analytics views to measure impact and outreach at each administrative level","Integrated authentication and authorization for admin, representatives, and public users","Optimized backend APIs for scalable data retrieval across large hierarchical datasets","Planned extensible architecture to support future integrations with analytics and AI modules"],tech:["Python","FastAPI","PostgreSQL","React","Next.js","Ant Design","Docker","REST APIs","Role-Based Access Control","Data Visualization"],github:"s",demo:"s",image:"/logo.png"},{id:"ai-grocery-meal-planner",title:"AI-Powered Grocery Prediction & Meal Planning System",summary:"Predictive grocery planning and weekly meal recommendation platform",description:"Developed an AI-driven application that predicts future grocery requirements and generates personalized weekly meal plans based on consumption history, preferences, and nutritional balance, helping users optimize shopping and reduce food waste.",bullets:["Designed data pipelines to capture grocery purchase history and meal consumption patterns","Built machine learning models to predict grocery requirements for upcoming weeks","Implemented recommendation logic for generating balanced weekly meal plans","Integrated rule-based and ML-driven approaches for dietary preferences and constraints","Developed APIs for meal plan generation and grocery list prediction","Built interactive dashboards to visualize predicted grocery quantities and meal schedules","Implemented feedback loops to improve prediction accuracy over time","Optimized models for scalability and real-time inference","Designed modular architecture to support future integrations with nutrition APIs","Built a user-friendly interface for meal planning and grocery tracking"],tech:["Python","Machine Learning","Pandas","Scikit-learn","FastAPI","Streamlit","PostgreSQL","Data Analytics","Recommendation Systems","Time Series Forecasting"],github:"s",demo:"s",image:"/ai.jpg"}])}];

//# sourceMappingURL=%5Broot-of-the-server%5D__dbadaaab._.js.map